# scripts/content_generator.py
import feedparser
import requests
from bs4 import BeautifulSoup
import os
import json
import yaml
from datetime import datetime
from anthropic import Anthropic
import time
import re
from urllib.parse import urljoin, urlparse
from knowledge_accumulation import LiberationKnowledgeSystem, extract_location_from_article, extract_domain_from_article, extract_themes_from_article

# Initialize Claude API and Knowledge System
client = Anthropic(api_key=os.environ['CLAUDE_API_KEY'])
knowledge_system = LiberationKnowledgeSystem()

# RSS Feeds to monitor
RSS_FEEDS = [
    "https://billypenn.com/feed",
    "https://wtop.com/feed",
    "https://www.kxan.com/feed",
    "https://ktla.com/feed",
    "https://wgntv.com/feed",
    "https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml",
    "https://www.npr.org/rss/rss.php?id=1001",
    "https://feeds.foxnews.com/foxnews/latest",
    "https://www.chicagotribune.com/arcio/rss/",
    "https://www.fox32chicago.com/rss/category/news",
    "https://blockclubchicago.org/feed/",
]

# Keywords to filter relevant articles
RELEVANT_KEYWORDS = [
    'police', 'housing', 'health', 'school', 'education', 'city council',
    'mayor', 'budget', 'development', 'community', 'neighborhood',
    'crime', 'safety', 'racism', 'discrimination', 'inequality', 'tenant',
    'eviction', 'gentrification', 'affordable', 'CPS', 'teacher', 'student'
]

# Quality assessment keywords
SYSTEMATIC_KEYWORDS = [
    'pattern', 'systematic', 'ongoing', 'historical', 'community impact',
    'residents', 'organizing', 'policy', 'decision', 'funding', 'resources',
    'government', 'officials', 'department', 'program', 'institution'
]

# Actor/Institution patterns for identification
ACTOR_PATTERNS = {
    'police_departments': [
        'police department', 'pd', 'law enforcement', 'sheriff', 'police chief',
        'chicago police', 'austin police', 'philadelphia police', 'lapd', 'nypd'
    ],
    'education_agencies': [
        'school district', 'education department', 'education agency', 'cps',
        'texas education agency', 'department of education', 'school board'
    ],
    'housing_agencies': [
        'housing authority', 'housing department', 'planning department',
        'zoning', 'development authority', 'cha', 'nycha'
    ],
    'health_agencies': [
        'health department', 'hospital system', 'health authority',
        'public health', 'medicaid', 'health services'
    ],
    'corporations': [
        'bank', 'corporation', 'company', 'llc', 'inc', 'developer',
        'goldman sachs', 'jpmorgan', 'wells fargo', 'amazon', 'walmart'
    ]
}

class ActorProfileManager:
    """Manages actor profiles and power structure mapping"""
    
    def __init__(self):
        self.actors_dir = 'actors'
        os.makedirs(self.actors_dir, exist_ok=True)
    
    def identify_actors_in_article(self, article):
        """Extract key institutional actors from article content"""
        content = article['content'].lower()
        title = article['title'].lower()
        text = content + ' ' + title
        
        identified_actors = []
        
        for category, patterns in ACTOR_PATTERNS.items():
            for pattern in patterns:
                if pattern in text:
                    # Extract more specific name if possible
                    sentences = re.split(r'[.!?]', article['content'])
                    for sentence in sentences:
                        if pattern in sentence.lower():
                            # Try to extract proper noun phrase containing the pattern
                            words = sentence.split()
                            for i, word in enumerate(words):
                                if pattern.split()[0] in word.lower():
                                    # Extract 2-4 word institutional name
                                    name_parts = []
                                    for j in range(max(0, i-2), min(len(words), i+3)):
                                        if words[j][0].isupper() or words[j].lower() in pattern.split():
                                            name_parts.append(words[j])
                                    
                                    if name_parts:
                                        actor_name = ' '.join(name_parts)
                                        identified_actors.append({
                                            'name': actor_name,
                                            'category': category,
                                            'context': sentence.strip(),
                                            'pattern_matched': pattern
                                        })
                                    break
                            break
        
        return identified_actors
    
    def load_existing_actor_profile(self, actor_name):
        """Load existing actor profile if it exists"""
        filename = self._get_actor_filename(actor_name)
        filepath = os.path.join(self.actors_dir, filename)
        
        if os.path.exists(filepath):
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
                # Parse YAML frontmatter if it exists
                if content.startswith('---'):
                    try:
                        parts = content.split('---', 2)
                        if len(parts) >= 3:
                            metadata = yaml.safe_load(parts[1])
                            content_body = parts[2].strip()
                            return {'metadata': metadata, 'content': content_body}
                    except:
                        pass
                return {'metadata': {}, 'content': content}
        return None
    
    def _get_actor_filename(self, actor_name):
        """Convert actor name to filename"""
        safe_name = re.sub(r'[^\w\s-]', '', actor_name.lower())
        safe_name = re.sub(r'\s+', '-', safe_name)
        return f"{safe_name}.md"
    
    def create_actor_profile_prompt(self, actor, article, existing_profile=None):
        """Create prompt for generating/updating actor profile"""
        
        existing_context = ""
        if existing_profile:
            existing_context = f"""
EXISTING PROFILE TO UPDATE:
---
{existing_profile['metadata']}
---
{existing_profile['content']}

INSTRUCTIONS: Update this profile with new information from the current article. Add new policies, controversies, or resistance activities. Update funding sources and key relationships if new information is provided.
"""
        
        return f"""
You are building ACTOR POWER PROFILES for liberation organizing intelligence.

{existing_context}

ACTOR TO PROFILE: {actor['name']}
CATEGORY: {actor['category']}
CONTEXT FROM ARTICLE: {actor['context']}
FULL ARTICLE: {article['title']}
{article['content'][:1500]}...

Create a comprehensive power profile using this structure:

---
actor_name: "{actor['name']}"
category: "{actor['category']}"
primary_location: "[City/State]"
last_updated: "{datetime.now().strftime('%Y-%m-%d')}"
threat_level: "[Low/Medium/High/Critical]"
organizing_priority: "[Low/Medium/High/Critical]"
tags: [list, of, relevant, tags]
---

# {actor['name']} - Power Profile

## Role in the System
[How this actor functions within systematic oppression - what domain they control]

## Key Leadership & Decision-Makers
- **[Title]:** [Name] - [Background, key decisions]
- **[Title]:** [Name] - [Background, key decisions]

## Power & Influence
### Decision-Making Authority
[What they can decide, approve, or veto]

### Resource Control
[Budget size, funding streams, resource allocation power]

### Policy Implementation
[What policies they enforce, how they interpret regulations]

## Systematic Control Mechanisms
### Current Policies & Practices
- **[Policy Name]:** [How it affects communities]
- **[Practice]:** [Impact on organizing/communities]

### Funding Sources & Financial Interests
- **Primary Funding:** [Where their money comes from]
- **Financial Conflicts:** [Corporate ties, profit incentives]
- **Budget Dependencies:** [What funding they rely on]

## Historical Pattern Analysis
### Past Controversial Actions
- **[Year]:** [What they did and community impact]
- **[Year]:** [What they did and community impact]

### Evolution of Tactics
[How their approach has changed over time using Dignity Lens eras]

## Community Resistance & Accountability
### Successful Organizing Against This Actor
- **[Campaign/Year]:** [What worked, what was achieved]
- **[Campaign/Year]:** [What worked, what was achieved]

### Current Vulnerabilities
- **Political Pressure Points:** [Elections, oversight, public pressure]
- **Legal Vulnerabilities:** [Potential legal challenges]
- **Economic Pressure:** [Funding dependencies, contracts]

### Failed Resistance (Lessons Learned)
[What hasn't worked and why]

## Strategic Organizing Opportunities
### Short-term (6 months)
[Immediate pressure campaigns, accountability measures]

### Medium-term (1-2 years)
[Electoral, policy, or structural changes possible]

### Long-term (3+ years)
[Fundamental power shifts, alternative institution building]

## Cross-City Connections
[How this actor connects to similar institutions in other cities]

## Intelligence Gaps
[What we need to learn more about for better organizing]

---

**Keep profiles ACTIONABLE for organizers - focus on power analysis and organizing opportunities, not just description.**
"""
    
    def save_actor_profile(self, actor_name, profile_content):
        """Save actor profile to file"""
        filename = self._get_actor_filename(actor_name)
        filepath = os.path.join(self.actors_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(profile_content)
        
        print(f"ðŸ’¼ Saved actor profile: {filepath}")
        return filepath

class TimelineGenerator:
    """Generates historical timelines using Dignity Lens eras"""
    
    def __init__(self):
        self.timelines_dir = 'timelines'
        os.makedirs(self.timelines_dir, exist_ok=True)
        
        # Dignity Lens Era Framework
        self.dignity_lens_eras = {
            'Era 1 (1600s-1865)': 'Enslavement & Early Resistance',
            'Era 2 (1865-1910)': 'Reconstruction & Backlash', 
            'Era 3 (1910-1950)': 'Jim Crow & Black Institution-Building',
            'Era 4 (1950-1975)': 'Civil Rights & Black Power',
            'Era 5 (1975-2008)': 'Neoliberalization & Mass Incarceration',
            'Era 6 (2008-2020)': 'Digital Rebellion & Corporate Capture',
            'Era 7 (2020-Present)': 'Abolitionist Futuring & AI Counterinsurgency'
        }
    
    def create_timeline_prompt(self, case_study_content, domain, location):
        """Create prompt for timeline generation"""
        
        return f"""
You are creating a DIGNITY LENS HISTORICAL TIMELINE for liberation organizing.

CASE STUDY CONTENT:
{case_study_content[:2000]}...

DOMAIN: {domain}
LOCATION: {location}

Create a timeline showing how this systematic issue has evolved through the Dignity Lens historical framework. Use this structure:

---
timeline_name: "{domain} in {location}"
domain: "{domain}"
location: "{location}"
created: "{datetime.now().strftime('%Y-%m-%d')}"
dignity_lens_eras: true
organizing_opportunities: true
---

# {domain} Timeline - {location}
*Evolution of Systematic Oppression and Community Resistance*

## Era Overview
Brief explanation of how this domain operates systematically in this location.

## Historical Timeline

### Era 1 (1600s-1865): Enslavement & Early Resistance
**Systematic Oppression:**
- [How this domain functioned during slavery]
- [Key policies, practices, institutions]

**Community Resistance:**
- [How communities fought back]
- [Early organizing strategies]

### Era 2 (1865-1910): Reconstruction & Backlash
**Systematic Oppression:**
- [How the system adapted after slavery]
- [New control mechanisms developed]

**Community Resistance:**
- [How communities organized during this period]
- [Institution-building efforts]

### Era 3 (1910-1950): Jim Crow & Black Institution-Building
**Systematic Oppression:**
- [Legal segregation in this domain]
- [How exclusion was formalized]

**Community Resistance:**
- [Independent institutions created]
- [Organizing strategies developed]

### Era 4 (1950-1975): Civil Rights & Black Power
**Systematic Oppression:**
- [How the system resisted integration]
- [New forms of exclusion developed]

**Community Resistance:**
- [Civil rights organizing in this domain]
- [Black Power alternatives created]

### Era 5 (1975-2008): Neoliberalization & Mass Incarceration
**Systematic Oppression:**
- [Privatization, disinvestment, criminalization]
- [How corporate control expanded]

**Community Resistance:**
- [Community organizing responses]
- [Alternative institution building]

### Era 6 (2008-2020): Digital Rebellion & Corporate Capture
**Systematic Oppression:**
- [How digital technology changed oppression]
- [Corporate platform control]

**Community Resistance:**
- [Digital organizing innovations]
- [New forms of community power]

### Era 7 (2020-Present): Abolitionist Futuring & AI Counterinsurgency
**Systematic Oppression:**
- [Current forms of algorithmic control]
- [New surveillance and control methods]

**Community Resistance:**
- [Current organizing strategies]
- [Community alternatives being built]

## Pattern Analysis
### Consistent Systematic Functions
[What functions have remained constant across eras]

### Adaptation Strategies
[How oppression has evolved its methods]

### Resistance Evolution  
[How community organizing has evolved]

## Strategic Organizing Opportunities
### Historical Lessons
[What past organizing teaches us]

### Current Vulnerabilities
[Where the system is weak now]

### Future Preparation
[How to prepare for system adaptation]

## Cross-City Connections
[How this timeline connects to similar patterns in other cities]

---

**Focus on ACTIONABLE historical patterns that inform current organizing strategy.**
"""
    
    def save_timeline(self, domain, location, timeline_content):
        """Save timeline to file"""
        safe_domain = re.sub(r'[^\w\s-]', '', domain.lower())
        safe_domain = re.sub(r'\s+', '-', safe_domain)
        safe_location = re.sub(r'[^\w\s-]', '', location.lower())
        safe_location = re.sub(r'\s+', '-', safe_location)
        
        filename = f"{safe_domain}-{safe_location}-timeline.md"
        filepath = os.path.join(self.timelines_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(timeline_content)
        
        print(f"ðŸ“… Saved timeline: {filepath}")
        return filepath

def fetch_full_article_content(url):
    """Fetch complete article content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Remove unwanted elements
        for element in soup(['script', 'style', 'nav', 'header', 'footer', 'aside', 'advertisement']):
            element.decompose()
        
        # Try multiple selectors for article content
        content_selectors = [
            'article', '.article-content', '.story-body', '.entry-content',
            '.post-content', '.content', '.story', 'main', '.article-body',
            '.article-text', '.story-content', '.post-body'
        ]
        
        article_content = ""
        title = ""
        
        # Get title
        title_element = soup.find('h1') or soup.find('title')
        if title_element:
            title = title_element.get_text(strip=True)
        
        # Get content
        for selector in content_selectors:
            content_div = soup.select_one(selector)
            if content_div:
                # Get text and clean it
                text = content_div.get_text(separator=' ', strip=True)
                if len(text) > 300:  # Minimum reasonable article length
                    article_content = text
                    break
        
        # If no content found with selectors, try paragraphs
        if not article_content:
            paragraphs = soup.find_all('p')
            if len(paragraphs) > 3:
                article_content = ' '.join([p.get_text(strip=True) for p in paragraphs])
        
        # Clean up the content
        article_content = re.sub(r'\s+', ' ', article_content)
        article_content = re.sub(r'Advertisement\s*', '', article_content)
        
        return {
            'title': title,
            'content': article_content,
            'url': url,
            'word_count': len(article_content.split())
        }
        
    except Exception as e:
        print(f"Error fetching full content from {url}: {e}")
        return None

def fact_check_content(article):
    """Fact-check article content for accuracy and context"""
    fact_check_prompt = f"""
FACT-CHECK ASSESSMENT for Dignity Lens Analysis:

ARTICLE: {article['title']}
URL: {article['url']}
CONTENT: {article['content'][:2000]}...

Please evaluate this content for:

1. FACTUAL ACCURACY:
- Are claims supported by evidence?
- Are statistics/data presented correctly?
- Are quotes and attributions accurate?
- Any obvious misinformation or bias?

2. CONTEXT COMPLETENESS:
- Missing important background information?
- One-sided presentation of complex issues?
- Historical context provided or absent?
- Community perspectives included or excluded?

3. SOURCE RELIABILITY:
- Credible publication/outlet?
- Author expertise on the topic?
- Primary sources vs. speculation?
- Corporate/political interests that might bias coverage?

4. SYSTEMATIC ANALYSIS READINESS:
- Sufficient detail for institutional analysis?
- Connects to broader patterns beyond individual incident?
- Relates to community organizing or policy implications?
- Addresses root causes vs. just symptoms?

RESPOND WITH:
- FACT_CHECK_PASS: Content is factually sound and suitable for Dignity Lens analysis
- FACT_CHECK_CONCERNS: Content has issues but may still be usable (explain concerns)
- FACT_CHECK_FAIL: Content is unreliable or insufficient for serious analysis

If PASS or CONCERNS, provide brief summary of any important context or background information that should be included in the analysis.

If FAIL, explain specific factual problems or missing context that makes this unsuitable for systematic racism analysis.
"""
    return fact_check_prompt

def assess_content_quality(article):
    """Determine if article has sufficient context for Dignity Lens analysis"""
    if not article or not article.get('content'):
        return {'sufficient': False, 'reason': 'No content'}
    
    content = article['content'].lower()
    word_count = article.get('word_count', 0)
    
    # Check minimum length
    if word_count < 200:
        return {'sufficient': False, 'reason': f'Too short ({word_count} words)'}
    
    # Check for systematic elements
    systematic_score = sum(1 for keyword in SYSTEMATIC_KEYWORDS if keyword in content)
    
    # Check for community context
    community_keywords = ['community', 'residents', 'neighborhood', 'families', 'people']
    community_score = sum(1 for keyword in community_keywords if keyword in content)
    
    # Check for institutional elements
    institution_keywords = ['city', 'government', 'department', 'official', 'policy', 'program']
    institution_score = sum(1 for keyword in institution_keywords if keyword in content)
    
    # Quality scoring
    total_score = systematic_score + community_score + institution_score
    
    if total_score >= 3 and word_count >= 300:
        return {'sufficient': True, 'score': total_score, 'word_count': word_count}
    else:
        return {'sufficient': False, 'reason': f'Insufficient context (score: {total_score}, words: {word_count})'}

def fetch_rss_articles():
    """Fetch articles from RSS feeds"""
    articles = []
    
    for feed_url in RSS_FEEDS:
        try:
            print(f"Fetching from {feed_url}...")
            feed = feedparser.parse(feed_url)
            for entry in feed.entries[:3]:  # Reduced to 3 per feed for quality
                articles.append({
                    'title': entry.title,
                    'content': getattr(entry, 'summary', ''),
                    'url': entry.link,
                    'published': getattr(entry, 'published', ''),
                    'source': feed_url
                })
        except Exception as e:
            print(f"Claude API error (attempt {attempt + 1}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)  # Wait before retry
            else:
                return None

def save_content(content, content_type, article_title, is_dual_case_study=False):
    """Save generated content to appropriate folder"""
    if not content:
        return
    
    # Skip if content indicates insufficient context
    if 'INSUFFICIENT_CONTEXT' in content or 'INSUFFICIENT_CONTENT' in content:
        print(f"Skipped {content_type}: {article_title[:50]} - insufficient content")
        return
    
    # Create directories if they don't exist
    os.makedirs(f'drafts/{content_type}', exist_ok=True)
    
    # Create filename
    date_str = datetime.now().strftime('%Y%m%d')
    safe_title = "".join(c for c in article_title if c.isalnum() or c in (' ', '-', '_')).rstrip()[:50]
    
    # For dual case studies, split and save separately
    if is_dual_case_study and content_type == 'case-studies':
        # Split the content on the separator
        if '# NATIONAL COMPARATIVE ANALYSIS:' in content:
            local_study, national_study = content.split('# NATIONAL COMPARATIVE ANALYSIS:', 1)
            
            # Save local case study
            local_filename = f'drafts/{content_type}/{date_str}-LOCAL-{safe_title}.md'
            with open(local_filename, 'w', encoding='utf-8') as f:
                f.write(local_study.strip())
            print(f"âœ… Saved Local: {local_filename}")
            
            # Save national comparative study
            national_filename = f'drafts/{content_type}/{date_str}-NATIONAL-{safe_title}.md'
            with open(national_filename, 'w', encoding='utf-8') as f:
                f.write('# NATIONAL COMPARATIVE ANALYSIS:' + national_study)
            print(f"âœ… Saved National: {national_filename}")
            return
    
    # Regular single file save
    filename = f'drafts/{content_type}/{date_str}-{safe_title}.md'
    
    # Save content
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"âœ… Saved: {filename}")

def main():
    """Main content generation function with actor profiles and timeline generation"""
    print("ðŸš€ Starting enhanced daily content generation with Actor Profiles and Timeline Generation...")
    
    # Initialize actor and timeline managers
    actor_manager = ActorProfileManager()
    timeline_generator = TimelineGenerator()
    
    # Fetch and filter articles
    print("ðŸ“° Fetching RSS articles...")
    articles = fetch_rss_articles()
    relevant_articles = filter_relevant_articles(articles)
    
    print(f"ðŸ“‹ Found {len(relevant_articles)} relevant articles")
    print("ðŸ” Fetching full article content and assessing quality...")
    
    processed_count = 0
    
    for i, article in enumerate(relevant_articles[:6]):  # Reduced for fact-checking overhead
        print(f"\nðŸ“– Processing article {i+1}: {article['title'][:60]}...")
        
        # Get full article content
        full_article = fetch_full_article_content(article['url'])
        if not full_article:
            print("âŒ Could not fetch full content")
            continue
        
        # Assess quality
        quality = assess_content_quality(full_article)
        if not quality['sufficient']:
            print(f"âš ï¸ Skipping - {quality['reason']}")
            continue
        
        print(f"âœ… Quality check passed - Score: {quality.get('score', 'N/A')}, Words: {quality.get('word_count', 'N/A')}")
        
        # Use full article for analysis
        enhanced_article = {**article, **full_article}
        
        # Fact-check the content
        print("ðŸ” Fact-checking content...")
        fact_check_prompt = fact_check_content(enhanced_article)
        fact_check_result = call_claude_api(fact_check_prompt)
        
        if fact_check_result and "FACT_CHECK_FAIL" in fact_check_result:
            print(f"âŒ Fact-check failed: {fact_check_result[:100]}...")
            continue
        
        if fact_check_result and "FACT_CHECK_PASS" in fact_check_result:
            print("âœ… Fact-check passed")
        elif fact_check_result and "FACT_CHECK_CONCERNS" in fact_check_result:
            print("âš ï¸ Fact-check has concerns - will address in analysis")
        
        # ACTOR PROFILE PROCESSING
        print("ðŸ’¼ Identifying and processing institutional actors...")
        identified_actors = actor_manager.identify_actors_in_article(enhanced_article)
        
        actor_profiles = {}
        for actor in identified_actors:
            print(f"   ðŸ›ï¸ Processing actor: {actor['name']}")
            
            # Load existing profile if available
            existing_profile = actor_manager.load_existing_actor_profile(actor['name'])
            
            # Generate/update actor profile
            actor_prompt = actor_manager.create_actor_profile_prompt(
                actor, enhanced_article, existing_profile
            )
            actor_profile_content = call_claude_api(actor_prompt)
            
            if actor_profile_content:
                # Save the profile
                actor_manager.save_actor_profile(actor['name'], actor_profile_content)
                
                # Store for case study context
                actor_profiles[actor['name']] = {
                    'content': actor_profile_content,
                    'metadata': actor  # Include identification metadata
                }
        
        # Generate dual case studies with actor context
        print("ðŸ§  Generating dual Dignity Lens case studies (Local + National) with Actor Intelligence...")
        case_study_prompt = create_enhanced_case_study_prompt(
            enhanced_article, fact_check_result, actor_profiles
        )
        case_study = call_claude_api(case_study_prompt)
        save_content(case_study, 'case-studies', enhanced_article['title'], is_dual_case_study=True)
        
        # TIMELINE GENERATION
        if case_study and not ('INSUFFICIENT_CONTEXT' in case_study or 'INSUFFICIENT_CONTENT' in case_study):
            print("ðŸ“… Generating historical timeline using Dignity Lens eras...")
            
            location = extract_location_from_article(enhanced_article)
            domain = extract_domain_from_article(enhanced_article)
            
            if location and domain:
                timeline_prompt = timeline_generator.create_timeline_prompt(
                    case_study, domain, location
                )
                timeline_content = call_claude_api(timeline_prompt)
                
                if timeline_content:
                    timeline_generator.save_timeline(domain, location, timeline_content)
            else:
                print("âš ï¸ Could not extract location/domain for timeline generation")
        
        # Generate news article
        print("ðŸ“° Generating community journalism article...")
        news_prompt = create_news_article_prompt(enhanced_article, fact_check_result)
        news_article = call_claude_api(news_prompt)
        save_content(news_article, 'news-articles', enhanced_article['title'])
        
        # Generate blog post
        print("ðŸ“ Generating accessible blog post...")
        blog_prompt = create_blog_post_prompt(enhanced_article, fact_check_result)
        blog_post = call_claude_api(blog_prompt)
        save_content(blog_post, 'blog-posts', enhanced_article['title'])
        
        processed_count += 1
        
        # Process knowledge accumulation from generated case study
        if case_study and not ('INSUFFICIENT_CONTEXT' in case_study or 'INSUFFICIENT_CONTENT' in case_study):
            print("ðŸ§  Accumulating knowledge from case study...")
            knowledge_system.process_case_study(
                title=enhanced_article['title'],
                content=case_study,
                city=extract_location_from_article(enhanced_article),
                domain=extract_domain_from_article(enhanced_article)
            )
        
        print(f"âœ… Completed article {i+1} with fact-checking, actor profiling, timeline generation, and knowledge accumulation")
    
    # Generate summary report
    print(f"\nðŸ“Š Generating daily intelligence summary...")
    generate_daily_summary_report(processed_count, actor_manager, timeline_generator)
    
    print(f"\nðŸŽ‰ Enhanced daily content generation complete!")
    print(f"   ðŸ“„ Processed {processed_count} fact-checked, high-quality articles")
    print(f"   ðŸ’¼ Updated actor profiles in /actors/ directory")
    print(f"   ðŸ“… Generated historical timelines in /timelines/ directory")
    print(f"   ðŸ§  Accumulated liberation organizing intelligence")

def generate_daily_summary_report(processed_count, actor_manager, timeline_generator):
    """Generate a daily intelligence summary"""
    date_str = datetime.now().strftime('%Y-%m-%d')
    
    # Count actor profiles
    actor_files = len([f for f in os.listdir(actor_manager.actors_dir) if f.endswith('.md')])
    
    # Count timelines
    timeline_files = len([f for f in os.listdir(timeline_generator.timelines_dir) if f.endswith('.md')])
    
    summary = f"""# Daily Liberation Intelligence Summary - {date_str}

## Processing Overview
- **Articles Processed:** {processed_count}
- **Actor Profiles:** {actor_files} total institutional actors tracked
- **Historical Timelines:** {timeline_files} systematic domain timelines generated
- **Knowledge Base:** Continuously accumulating liberation organizing intelligence

## Key Capabilities Enhanced Today
- âœ… **Actor Intelligence:** Institutional power mapping and vulnerability analysis
- âœ… **Historical Context:** Dignity Lens era-based timeline generation  
- âœ… **Fact-Checking:** Content verification for systematic racism analysis
- âœ… **Cross-City Patterns:** Multi-location organizing strategy identification
- âœ… **Knowledge Accumulation:** Building institutional memory for liberation organizing

## Intelligence Infrastructure
- `/actors/` - Institutional power profiles for organizing intelligence
- `/timelines/` - Historical evolution analysis using Dignity Lens framework
- `/drafts/case-studies/` - Local and national systematic analysis
- `/drafts/news-articles/` - Liberation-focused community journalism
- `/drafts/blog-posts/` - Accessible systematic racism education

## Strategic Value
This enhanced system now provides:
1. **Actor-specific organizing intelligence** (vulnerabilities, funding sources, past controversies)
2. **Historical pattern recognition** (how oppression evolved, what resistance worked)
3. **Cross-city coalition opportunities** (similar patterns across multiple locations)
4. **Real-time fact-checking** (content verification for organizing reliability)
5. **Accumulated institutional memory** (building knowledge faster than systems adapt)

*Liberation Technology: Communities building knowledge and strategy faster than oppression can evolve.*
"""
    
    # Save summary
    os.makedirs('reports', exist_ok=True)
    summary_path = f"reports/daily-summary-{date_str}.md"
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(summary)
    
    print(f"ðŸ“Š Daily summary saved: {summary_path}")

if __name__ == "__main__":
    main()
            print(f"Error fetching {feed_url}: {e}")
    
    return articles

def filter_relevant_articles(articles):
    """Filter articles for relevant content"""
    relevant = []
    
    for article in articles:
        text = (article['title'] + ' ' + article['content']).lower()
        if any(keyword in text for keyword in RELEVANT_KEYWORDS):
            relevant.append(article)
    
    return relevant

def create_enhanced_case_study_prompt(article, fact_check_result=None, actor_profiles=None):
    """Create enhanced prompt for Dignity Lens case study with actor profiles context"""
    
    # Get accumulated knowledge context
    location = extract_location_from_article(article)
    domain = extract_domain_from_article(article) 
    themes = extract_themes_from_article(article)
    
    context = knowledge_system.get_relevant_context(location, domain, themes)
    
    # Build actor context
    actor_context = ""
    if actor_profiles:
        actor_context = "\n\nKNOWN INSTITUTIONAL ACTORS:\n"
        for actor_name, profile in actor_profiles.items():
            actor_context += f"\n**{actor_name}:**\n"
            # Extract key info from profile metadata if available
            if 'metadata' in profile and profile['metadata']:
                metadata = profile['metadata']
                if 'threat_level' in metadata:
                    actor_context += f"- Threat Level: {metadata['threat_level']}\n"
                if 'organizing_priority' in metadata:
                    actor_context += f"- Organizing Priority: {metadata['organizing_priority']}\n"
            # Add brief content excerpt
            content_excerpt = profile.get('content', '')[:300]
            actor_context += f"- Background: {content_excerpt}...\n"
    
    # Build context section
    knowledge_context = ""
    if context['power_structures']:
        knowledge_context += f"\n\nKNOWN POWER STRUCTURES in {location or 'this region'}:\n"
        for ps in context['power_structures']:
            knowledge_context += f"- Key Actors: {', '.join(ps.get('key_actors', [])[:5])}\n"
            knowledge_context += f"- Institutions: {', '.join(ps.get('institutions', [])[:5])}\n"
            knowledge_context += f"- Control Mechanisms: {', '.join(ps.get('control_mechanisms', [])[:5])}\n"
    
    if context['organizing_strategies']:
        knowledge_context += f"\n\nPROVEN ORGANIZING STRATEGIES from our knowledge base:\n"
        for strategy in context['organizing_strategies'][:10]:
            knowledge_context += f"- {strategy.get('name', 'Unknown')}\n"
    
    if context['similar_cases']:
        knowledge_context += f"\n\nSIMILAR CASES in our database:\n"
        for case in context['similar_cases'][:5]:
            knowledge_context += f"- {case.get('title', 'Unknown')} ({case.get('city', 'Unknown city')})\n"
    
    fact_check_context = ""
    if fact_check_result and "FACT_CHECK_PASS" in fact_check_result:
        fact_check_context = f"\n\nFACT-CHECK CLEARED: This content has been verified for accuracy and systematic analysis readiness.\n{fact_check_result}\n"
    elif fact_check_result and "FACT_CHECK_CONCERNS" in fact_check_result:
        fact_check_context = f"\n\nFACT-CHECK NOTES: Please address these concerns in your analysis:\n{fact_check_result}\n"
    
    return f"""
You are DignityAI with access to accumulated institutional memory and actor intelligence. Create TWO complementary case studies using the Dignity Lens framework.
{fact_check_context}
{knowledge_context}
{actor_context}

ARTICLE TO ANALYZE:
Title: {article['title']}
Content: {article['content']}
Word Count: {article.get('word_count', 'Unknown')}
Source: {article['url']}

GENERATE TWO CASE STUDIES WITH ENHANCED ACTOR ANALYSIS:

## CASE STUDY A: LOCAL SYSTEMATIC ANALYSIS
Focus specifically on the city/region mentioned in the article. Use the accumulated knowledge and actor profiles above to provide deeper context about known power structures and proven organizing strategies in this location.

## CASE STUDY B: NATIONAL COMPARATIVE ANALYSIS  
Compare this local pattern to how the same systematic issue operates in other major cities. Reference similar cases from our knowledge base and actor profiles when relevant.

Format as markdown with:

# LOCAL CASE STUDY: [City Name] - [Issue]
## Executive Summary
## Institutional Memory Context
[Reference relevant power structures and past organizing from our knowledge base]
## Actor Power Analysis
[Reference specific institutional actors from our profiles - their role, vulnerabilities, past controversies]
## Local Dignity Lens Analysis
### Power Structures (City/County/State Level)
### Control Mechanisms (How This Operates Locally)
### Community Resistance (Local Organizing Examples)
### Liberation Strategies (What's Worked in This City)
## Local Organizing Opportunities
[Be specific about which actors to target and how]
## Conclusion

---

# NATIONAL COMPARATIVE ANALYSIS: [Issue] Across U.S. Cities
## Executive Summary
## Cross-City Pattern Analysis
[Reference similar cases from knowledge base]
### How Power Structures Vary by Region
### Control Mechanisms: Common Tactics vs. Local Variations
### Community Resistance: Successful Models from Different Cities
### Liberation Strategies: What's Replicable Nationwide
## Actor Network Analysis
[How institutional actors connect across cities - shared tactics, funding, leadership]
## State-by-State Policy Comparisons
## Federal Policy Connections
## Cross-City Organizing Opportunities
### Regional Coalition Building
### National Movement Connections
### Policy Advocacy at Multiple Levels
## Knowledge Base Integration
[How this case adds to our understanding of systematic patterns and actor networks]
## Conclusion

Each case study should be 1000-1200 words. Use accumulated institutional memory and actor profiles to provide deeper analysis and more specific organizing recommendations based on proven strategies and known actor vulnerabilities.
"""

def create_news_article_prompt(article, fact_check_result=None):
    """Create prompt for community journalism article with fact-check integration"""
    fact_check_context = ""
    if fact_check_result:
        fact_check_context = f"\n\nFACT-CHECK NOTES: {fact_check_result}\n"
    
    return f"""
You are a Liberation Technology journalist for the People's Newsroom.
{fact_check_context}
ARTICLE:
Title: {article['title']}
Content: {article['content']}
Source: {article['url']}

Rewrite from a MULTI-CITY LIBERATION ORGANIZING perspective (700-900 words):
- Center community voices and impacts in the specific city/region
- Analyze systematic patterns that operate across multiple cities
- Connect local story to broader liberation organizing nationwide
- Compare similar organizing strategies in Chicago, LA, Austin, Philadelphia, DC
- Provide community organizing context that can be replicated
- Include actionable information for residents locally and regionally
- Address any fact-check concerns noted above

Format as:
# [Community-Centered Headline]
## [Subheading focusing on systematic patterns across cities]

[Article content with multi-city Liberation Technology perspective]

**Fact-Check and Sources:**
[Any additional context or verification needed]

**Community Organizing Opportunities:**

**Local Actions (City-Specific):**
- [Actions for residents in this specific city]
- [Local organizations to connect with]
- [City council meetings, local campaigns]

**Regional and National Connections:**
- [How this connects to organizing in other cities]
- [Cross-city coalitions and networks]
- [Federal policy implications and actions]
"""

def create_blog_post_prompt(article, fact_check_result=None):
    """Create prompt for accessible blog post with fact-check integration"""
    fact_check_context = ""
    if fact_check_result:
        fact_check_context = f"\n\nFACT-CHECK NOTES: {fact_check_result}\n"
    
    return f"""
You are writing for DRC's blog to make systematic racism analysis accessible.
{fact_check_context}
ARTICLE:
Title: {article['title']}
Content: {article['content']}
Source: {article['url']}

Create an accessible blog post (500-700 words) that makes MULTI-CITY SYSTEMATIC PATTERNS accessible:
- Makes systematic racism analysis accessible to general readers
- Uses concrete examples from the news story
- Connects individual story to patterns across Chicago, LA, Austin, Philadelphia, DC
- Shows how the same systems operate in different cities
- Provides hope and organizing pathways that can be replicated
- Addresses any fact-check concerns

Format as:
# [Engaging, accessible headline that shows broader pattern]

[Content explaining systematic patterns through this example]

## What This Really Means
[Connect local story to systematic patterns across multiple cities]

## The Full Context
[Address any fact-check concerns or missing context]

## How This Shows Up in Other Cities
[Examples of similar patterns in LA, Chicago, Austin, Philadelphia, etc.]

## What We Can Do About It
[Organizing opportunities locally and regionally]

**Get Involved:**

**In Your City:**
- [Local actions relevant to the city discussed]
- [Local organizations to support]

**Regional and National:**
- [Cross-city organizing opportunities]
- [National networks and coalitions]
"""

def call_claude_api(prompt, max_retries=3):
    """Send prompt to Claude API with retry logic"""
    for attempt in range(max_retries):
        try:
            message = client.messages.create(
                model="claude-3-5-haiku-20241022",
                max_tokens=4000,
                messages=[{"role": "user", "content": prompt}]
            )
            # Rate limiting - stay under API limits
            time.sleep(1.5)
            return message.content[0].text
        except Exception as e:

name: LegiScan Illinois Bulk Import

on:
  workflow_dispatch:
  schedule:
    - cron: '0 2 * * 1'  # every Monday at 2AM UTC

jobs:
  legiscan-bulk:
    runs-on: ubuntu-latest

    env:
      GCS_BUCKET: ${{ secrets.GCS_BUCKET }}
      GCS_CREDENTIALS: ${{ secrets.GCS_CREDENTIALS_JSON }}
      S3_BUCKET: ${{ secrets.S3_BUCKET }}
      AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
      AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      LEGISCAN_API_KEY: ${{ secrets.LEGISCAN_API_KEY }}

    steps:
    - name: Checkout repo
      uses: actions/checkout@v3

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: 3.10

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install google-cloud-storage boto3

    - name: Download Illinois datasets (fill your own download method here)
      run: |
        mkdir -p datasets
        # Example: wget or curl your ZIPs from LegiScan API or storage
        # Replace with actual download commands using $LEGISCAN_API_KEY
        # wget "https://example.com/illinois_dataset_2025.zip" -O datasets/illinois_2025.zip
        echo "Download step - insert your download logic here"

    - name: Run unzip and generate markdown summaries
      run: |
        python unzip_and_organize.py

    - name: Check and upload large files to GCS or S3
      run: |
        # Upload files > 50MB to GCS
        python -c "
import os
from google.cloud import storage
import boto3

THRESHOLD = 50 * 1024 * 1024  # 50MB
upload_to_gcs = os.getenv('GCS_BUCKET') is not None
upload_to_s3 = os.getenv('S3_BUCKET') is not None

def upload_gcs(local_path, bucket_name):
    client = storage.Client.from_service_account_info(${GCS_CREDENTIALS})
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(os.path.basename(local_path))
    blob.upload_from_filename(local_path)
    print(f'Uploaded {local_path} to GCS bucket {bucket_name}')

def upload_s3(local_path, bucket_name):
    s3 = boto3.client('s3')
    s3.upload_file(local_path, bucket_name, os.path.basename(local_path))
    print(f'Uploaded {local_path} to S3 bucket {bucket_name}')

for root, dirs, files in os.walk('output/IL'):
    for f in files:
        path = os.path.join(root, f)
        size = os.path.getsize(path)
        if size > THRESHOLD:
            if upload_to_gcs:
                upload_gcs(path, os.getenv('GCS_BUCKET'))
            elif upload_to_s3:
                upload_s3(path, os.getenv('S3_BUCKET'))
            else:
                print(f'Large file found but no cloud bucket configured: {path}')
"

    - name: Commit and push markdown files
      run: |
        git config --global user.name "github-actions[bot]"
        git config --global user.email "github-actions[bot]@users.noreply.github.com"
        git add output/IL/*.md
        git commit -m "Update Illinois bill summaries"
        git push

